# model.py
import tensorflow as tf

def build_model(input_dim):
    # model = tf.keras.Sequential([
    #     tf.keras.layers.InputLayer(input_shape=(input_dim,)),
    #     tf.keras.layers.Dense(128, activation='relu'),
    #     tf.keras.layers.Dropout(0.3),
    #     tf.keras.layers.Dense(64, activation='relu'),
    #     tf.keras.layers.Dense(1, activation='sigmoid')
    # ])

    # optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)

    # model.compile(
    #     loss='binary_crossentropy',
    #     optimizer=optimizer,
    #     metrics=['accuracy']
    # )

    model = tf.keras.Sequential([

        # ğŸ”¹ Input Layer
        # Accepts feature vectors of shape (num_features,)
        tf.keras.layers.InputLayer(input_shape=(input_dim,)),

        # ğŸ”¹ Dense Layer 1
        # 256 neurons with ReLU activation to learn rich, high-level representations
        tf.keras.layers.Dense(256, activation='relu'),
        
        # ğŸ”¹ Dropout Layer 1
        # Drop 30% of neurons during training to prevent overfitting
        tf.keras.layers.Dropout(0.3),

        # ğŸ”¹ Dense Layer 2
        # Further reduce to 128 neurons to compress learned features
        tf.keras.layers.Dense(128, activation='relu'),
        
        # ğŸ”¹ Dropout Layer 2
        # Additional regularization to improve generalization
        tf.keras.layers.Dropout(0.3),

        # ğŸ”¹ Dense Layer 3
        # 64 neurons to refine mid-level feature representation
        tf.keras.layers.Dense(64, activation='relu'),

        # ğŸ”¹ Dense Layer 4
        # 32 neurons to prepare features for final prediction
        tf.keras.layers.Dense(32, activation='relu'),

        # ğŸ”¹ Output Layer
        # Single neuron with sigmoid activation for binary classification output (between 0 and 1)
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])


    # âš™ï¸ Optimizer Configuration
    # Adam is an adaptive learning rate optimizer known for combining the benefits of RMSProp and SGD with momentum.
    # It adjusts the learning rate dynamically based on first- and second-order moments of the gradients.
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
    # ğŸ” Learning rate set to 1e-4 for stable and fine-grained convergence.

    # ğŸ§  Model Compilation
    # Binary cross-entropy is ideal for binary classification tasks where labels are 0 or 1.
    # Accuracy is used as a basic performance metric.
    model.compile(
        loss='binary_crossentropy',   # ğŸ¯ Objective: Minimize the difference between predicted and true labels
        optimizer=optimizer,          # ğŸ”§ Optimizer: Adam (adaptive and efficient)
        metrics=['accuracy']          # ğŸ“ˆ Metric: Accuracy (fraction of correct predictions)
    )

    return model

